# xls-r

## Self-training for ASR

[self-training-for-asr-hy](https://github.com/arampacha/xls-r/self-training-for-asr-pseudo-labeling.ipynb) notebook covers comlete 1st iteration of Noizy Student training for Armenian language. The training is done using HuggingFaceðŸ¤—. LM boosted decoding is based on [pyctcdecode](https://github.com/kensho-technologies/pyctcdecode).

This charts generated by the above notebook illustrate the effect of self-training. 1st iteration results in relative imrovement 18.2% and 28.4% for WER and loss respectively.

![eval loss](./eval_loss.svg)
![eval wer](./eval_wer.svg)

[self-training-for-asr-pseudo-labeling](https://github.com/arampacha/xls-r/self-training-for-asr-hy.ipynb) notebook covers pseudo-labeled dataset generation.

Noizy student can be repeated multiple times having potential for further performance improvement. A larger model based on `wav2vec2-xls-r-1b` trained for 4 iteration using this approach is available [here](https://huggingface.co/arampacha/wav2vec2-xls-r-1b-hy). It achieves 10.81 WER with LM boosted decoding (the best open-source armenian ASR model to my knowledge).